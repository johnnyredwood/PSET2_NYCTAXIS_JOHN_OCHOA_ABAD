{"block_file": {"custom/export_secrets_notebook_negocio.py:custom:python:export secrets notebook negocio": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\nimport os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n\n@custom\ndef export_snowflake_secrets(*args, **kwargs):\n\n    secrets = [\n        'SNOWFLAKE_USER_TRANSFORMER',\n        'SNOWFLAKE_PASS',\n        'ACCOUNT_ID',\n        'WAREHOUSE',\n        'DATABASE',\n        'SCHEMA',\n    ]\n\n    for key in secrets:\n        value = get_secret_value(key)\n        if value:\n            os.environ[key] = value\n\n    print(\"Secrets exportados\")\n", "file_path": "custom/export_secrets_notebook_negocio.py", "language": "python", "type": "custom", "uuid": "export_secrets_notebook_negocio"}, "data_exporters/exportador.py:data_exporter:python:exportador": {"content": "import pandas as pd\nfrom snowflake.connector import connect\nfrom snowflake.connector.pandas_tools import write_pandas\nimport os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_data_to_snowflake(*args, **kwargs):\n    \"\"\"\n    Exporta todos los batches del generador a Snowflake.\n    \"\"\"\n    # Debug: ver todas las keys disponibles en kwargs\n    print(\"\ud83d\udd0d Keys disponibles en kwargs:\", list(kwargs.keys()))\n    \n    # Buscar el generador en diferentes keys posibles\n    data_generator = None\n    possible_keys = ['output_0', 'df', 'output', 'data']\n    \n    for key in possible_keys:\n        if key in kwargs and kwargs[key] is not None:\n            data_generator = kwargs[key]\n            print(f\"\u2705 Generador encontrado en key: {key}\")\n            break\n    \n    if data_generator is None:\n        # Mostrar todas las keys y valores para debug\n        print(\"\ud83d\udccb Todos los kwargs:\")\n        for key, value in kwargs.items():\n            print(f\"   {key}: {type(value)}\")\n        raise ValueError(\"No se encontr\u00f3 el generador en ninguna key conocida\")\n    \n    # Configuraci\u00f3n de Snowflake\n    config = {\n        'user': os.getenv('SNOWFLAKE_USER'),\n        'password': os.getenv('SNOWFLAKE_PASSWORD'),\n        'account': os.getenv('SNOWFLAKE_ACCOUNT'),\n        'warehouse': os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n        'database': os.getenv('SNOWFLAKE_DATABASE'),\n        'schema': os.getenv('SNOWFLAKE_SCHEMA', 'PUBLIC')\n    }\n    \n    # Verificar configuraci\u00f3n\n    for key in ['user', 'password', 'account', 'database']:\n        if not config[key]:\n            raise ValueError(f\"Variable {key} no configurada en environment variables\")\n    \n    # Conectar a Snowflake\n    print(\"\ud83d\udd17 Conectando a Snowflake...\")\n    connection = connect(**config)\n    \n    total_batches = 0\n    total_rows = 0\n    \n    try:\n        print(\"\ud83d\ude80 Iniciando exportaci\u00f3n a Snowflake...\")\n        \n        # Procesar cada batch del generador\n        for batch_df in data_generator:\n            total_batches += 1\n            \n            # Exportar el batch actual\n            success, nchunks, nrows, _ = write_pandas(\n                conn=connection,\n                df=batch_df,\n                table_name='yellow_taxi_trips',\n                auto_create_table=True,\n                overwrite=False\n            )\n            \n            total_rows += nrows\n            print(f\"\u2705 Batch {total_batches}: {nrows} filas exportadas\")\n        \n        print(f\"\ud83c\udf89 Exportaci\u00f3n completada: {total_batches} batches, {total_rows} filas\")\n        \n    except Exception as e:\n        print(f\"\u274c Error: {e}\")\n        raise\n    finally:\n        connection.close()\n        print(\"\ud83d\udd12 Conexi\u00f3n cerrada\")", "file_path": "data_exporters/exportador.py", "language": "python", "type": "data_exporter", "uuid": "exportador"}, "data_exporters/exporter_taxizones.py:data_exporter:python:exporter taxizones": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom pandas import DataFrame\nfrom os import path\nfrom snowflake.connector.pandas_tools import write_pandas\nimport snowflake.connector\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nSNOWFLAKE_ACCOUNT = get_secret_value('ACCOUNT_ID')\nSNOWFLAKE_USER = get_secret_value('SNOWFLAKE_USER_TRANSFORMER')\nSNOWFLAKE_PASSWORD = get_secret_value('SNOWFLAKE_PASS')\nSNOWFLAKE_DATABASE = get_secret_value('DATABASE')\nSNOWFLAKE_SCHEMA = get_secret_value('SCHEMA')\nSNOWFLAKE_WAREHOUSE = get_secret_value('WAREHOUSE')\nSNOWFLAKE_TABLE = 'NEWYORK_TAXIS_ZONES'\n\ndef get_snowflake_conn():\n    return snowflake.connector.connect(\n        user=SNOWFLAKE_USER,\n        password=SNOWFLAKE_PASSWORD,\n        account=SNOWFLAKE_ACCOUNT,\n        warehouse=SNOWFLAKE_WAREHOUSE,\n        database=SNOWFLAKE_DATABASE,\n        schema=SNOWFLAKE_SCHEMA\n    ) #retorno de la conexi\u00f3n con Snowflake\n\n@data_exporter\ndef export_data_to_snowflake(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Snowflake warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#snowflake\n    \"\"\"\n\n    try:\n        conn = get_snowflake_conn() \n        write_pandas(conn=conn,df=df,table_name=SNOWFLAKE_TABLE,auto_create_table=True)\n        print(\"Data enviada correctamente a Snowflake para Zonas de Taxis de NY\")\n\n    except Exception as e:\n        print(f\"Error al cargar el archivo a Snowflake: {e}\")\n        raise\n\n", "file_path": "data_exporters/exporter_taxizones.py", "language": "python", "type": "data_exporter", "uuid": "exporter_taxizones"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/dataloader_taxizones.py:data_loader:python:dataloader taxizones": {"content": "import pandas as pd\nimport requests\nfrom io import StringIO\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nTAXI_ZONE_URL = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n\n@data_loader\ndef load_taxi_zone_data(output_green):\n    \"\"\"\n    Data loader para crear DataFrame con datos del taxi_zone_lookup.csv\n    \"\"\"\n    print(f\"Descargando archivo CSV de: {TAXI_ZONE_URL}\")\n    \n    try:\n        # Descargar el archivo CSV\n        resp = requests.get(TAXI_ZONE_URL, timeout=60)\n        resp.raise_for_status()\n        \n        # Leer el CSV en un DataFrame\n        df = pd.read_csv(StringIO(resp.text))\n        \n        print(f\"Archivo de zonas de taxis bajado exitosamente\")\n        print(f\"Columnas: {list(df.columns)}\")\n\n        return df\n            \n    except Exception as e:\n        print(f\"Error al descargar el archivo: {e}\")\n        raise", "file_path": "data_loaders/dataloader_taxizones.py", "language": "python", "type": "data_loader", "uuid": "dataloader_taxizones"}, "data_loaders/loader_and_exporter_taxis_green.py:data_loader:python:loader and exporter taxis green": {"content": "import pandas as pd\nimport requests\nfrom io import BytesIO\nimport pyarrow.parquet as pq\nfrom datetime import datetime\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nimport json\nimport os\n\nCHECKPOINT_FILE = \"checkpointTaxisGreen.json\"\n\ndef save_checkpoint(year, month):\n    with open(CHECKPOINT_FILE, \"w\") as f:\n        json.dump({\"year\": year, \"month\": month}, f)\n\ndef load_checkpoint():\n    if os.path.exists(CHECKPOINT_FILE):\n        with open(CHECKPOINT_FILE, \"r\") as f:\n            return json.load(f)\n    return {\"year\": 0, \"month\": 0}\n\n# Datos para la conexi\u00f3n con Snowflake. Ver si se pueden poner en el io_config.yaml\nSNOWFLAKE_ACCOUNT = get_secret_value('ACCOUNT_ID')\nSNOWFLAKE_USER = get_secret_value('SNOWFLAKE_USER_TRANSFORMER')\nSNOWFLAKE_PASSWORD = get_secret_value('SNOWFLAKE_PASS')\nSNOWFLAKE_DATABASE = get_secret_value('DATABASE')\nSNOWFLAKE_SCHEMA = get_secret_value('SCHEMA')\nSNOWFLAKE_WAREHOUSE = get_secret_value('WAREHOUSE')\nSNOWFLAKE_TABLE = 'NY_TAXIS_GREEN_BRONZE' \n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\" #Tramo gen\u00e9rico de URL que contiene los datos\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0\",\n    \"Accept\": \"application/octet-stream\"\n}\n\n# Cargar checkpoint\ncheckpoint = load_checkpoint()\nREANUDACION_YEAR = checkpoint[\"year\"]\nREANUDACION_MONTH = checkpoint[\"month\"]\n\ndef get_snowflake_conn():\n    return snowflake.connector.connect(\n        user=SNOWFLAKE_USER,\n        password=SNOWFLAKE_PASSWORD,\n        account=SNOWFLAKE_ACCOUNT,\n        warehouse=SNOWFLAKE_WAREHOUSE,\n        database=SNOWFLAKE_DATABASE,\n        schema=SNOWFLAKE_SCHEMA\n    ) #retorno de la conexi\u00f3n con Snowflake\n\ndef create_table_with_constraints(conn):\n    create_table_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {SNOWFLAKE_TABLE} (\n    VENDORID NUMBER,\n    LPEP_PICKUP_DATETIME NUMBER,\n    LPEP_DROPOFF_DATETIME NUMBER,\n    PASSENGER_COUNT NUMBER,\n    TRIP_DISTANCE FLOAT,\n    RATECODEID NUMBER,\n    STORE_AND_FWD_FLAG VARCHAR(1),\n    PULOCATIONID NUMBER,\n    DOLOCATIONID NUMBER,\n    PAYMENT_TYPE NUMBER,\n    FARE_AMOUNT FLOAT,\n    EXTRA FLOAT,\n    MTA_TAX FLOAT,\n    TIP_AMOUNT FLOAT,\n    TOLLS_AMOUNT FLOAT,\n    EHAIL_FEE FLOAT,\n    IMPROVEMENT_SURCHARGE FLOAT,\n    TOTAL_AMOUNT FLOAT,\n    TRIP_TYPE NUMBER,\n    CONGESTION_SURCHARGE FLOAT,\n    AIRPORT_FEE FLOAT,\n    RUN_ID NUMBER,\n    VENTANA_TEMPORAL NUMBER,\n    LOTE_MES VARCHAR(50),\n    MONTH NUMBER,\n    YEAR NUMBER,\n    PRIMARY KEY (LPEP_PICKUP_DATETIME, LPEP_DROPOFF_DATETIME, PULOCATIONID, DOLOCATIONID)\n    )\n    \"\"\"\n    cursor = conn.cursor() #Nos aseguramos idempotencia definiendo tabla con su llave primaria\n    cursor.execute(create_table_sql)\n    cursor.close()\n\n@data_loader\ndef load_and_export_data(output_yellow):\n    taxi_type = \"green\" #generamos segundo la tabla para taxis verdes\n    ID = 1\n    years = range(2015, 2026) #Carga de todos los datos de 2015-2025\n    months = range(1, 13) #Carga de todos los datos de todos los meses\n     \n    conn = get_snowflake_conn() # Crear conexi\u00f3n con Snowflake\n    \n    try:\n\n        create_table_with_constraints(conn)\n\n        for year in years: #iteramos por cada a\u00f1o por cada mes\n            if REANUDACION_YEAR != 0 and year < REANUDACION_YEAR:\n                print(f\"Saltando a\u00f1o {year} (ya procesado)\")\n                continue\n            for month in months:\n                if REANUDACION_YEAR == year and month <= REANUDACION_MONTH:\n                    print(f\"Saltando {year}-{month:02d} (ya procesado)\")\n                    continue\n                if year==2025 and month>7:\n                    break #Hasta aqui hay datos en la pagina de NEW YORK\n                file_name = f\"{taxi_type}_tripdata_{year}-{month:02d}.parquet\"\n                url = BASE_URL + file_name #URL para cada mes para cada a\u00f1o\n\n                print(f\"Descargando archivo parquet de: {url}\")\n                resp = requests.get(url, headers=HEADERS, timeout=60)\n                resp.raise_for_status()\n\n                parquet_file = pq.ParquetFile(BytesIO(resp.content)) #Lectura de datos de parquet\n                lote = 1\n\n                for batch in parquet_file.iter_batches(batch_size=1000000): #Iteraci\u00f3n por batches de datos para procesamiento sin hacer kill a la RAM\n                    df = batch.to_pandas()\n\n                    # Agregar metadatos solicitados en Deber para esquema Bronce\n                    df['run_id'] = ID\n                    df['ventana_temporal'] = datetime.now()\n                    df['lote_mes'] = f\"{lote}/{month}\"\n                    df['month'] = month\n                    df['year'] = year\n\n                    # Eliminar columnas problem\u00e1ticas directamente\n\n                    if 'cbd_congestion_fee' in df.columns:\n                        df = df.drop(columns=['cbd_congestion_fee']) #Eliminar columna que solo existe en pocos archivos\n\n                    df.columns = [c.upper() for c in df.columns] #Homogeneizar nombres de columnas\n                    \n                    # Exportar directamente datos a tabla de Snowflake\n                    from snowflake.connector.pandas_tools import write_pandas\n                    write_pandas(\n                        conn=conn,\n                        df=df,\n                        table_name=SNOWFLAKE_TABLE,\n                        auto_create_table=True\n                    )\n\n                    print(f\"{file_name}: lote {lote}, {len(df)} filas exportadas correctamente\")\n                    lote += 1\n\n                # Guardar checkpoint despu\u00e9s de cada mes\n                save_checkpoint(year, month)\n                print(f\"Checkpoint guardado: {year}-{month:02d}\")\n                ID += 1\n\n    except Exception as e:\n        print(f\"Error en carga de datos: {e}\")\n    finally:\n        conn.close()\n\n    return \"Procesamiento completado. Todo lo de taxis se encuentra en Snowflake\"", "file_path": "data_loaders/loader_and_exporter_taxis_green.py", "language": "python", "type": "data_loader", "uuid": "loader_and_exporter_taxis_green"}, "data_loaders/loader_and_exporter_taxis_yellow.py:data_loader:python:loader and exporter taxis yellow": {"content": "import pandas as pd\nimport requests\nfrom io import BytesIO\nimport pyarrow.parquet as pq\nfrom datetime import datetime\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nimport json\nimport os\n\nCHECKPOINT_FILE = \"checkpointTaxisYellow.json\"\n\ndef save_checkpoint(year, month):\n    with open(CHECKPOINT_FILE, \"w\") as f:\n        json.dump({\"year\": year, \"month\": month}, f)\n\ndef load_checkpoint():\n    if os.path.exists(CHECKPOINT_FILE):\n        with open(CHECKPOINT_FILE, \"r\") as f:\n            return json.load(f)\n    return {\"year\": 0, \"month\": 0}\n\n# Datos para la conexi\u00f3n con Snowflake. Ver si se pueden poner en el io_config.yaml\nSNOWFLAKE_ACCOUNT = get_secret_value('ACCOUNT_ID')\nSNOWFLAKE_USER = get_secret_value('SNOWFLAKE_USER_TRANSFORMER')\nSNOWFLAKE_PASSWORD = get_secret_value('SNOWFLAKE_PASS')\nSNOWFLAKE_DATABASE = get_secret_value('DATABASE')\nSNOWFLAKE_SCHEMA = get_secret_value('SCHEMA')\nSNOWFLAKE_WAREHOUSE = get_secret_value('WAREHOUSE')\nSNOWFLAKE_TABLE = 'NY_TAXIS_YELLOW_BRONZE' \n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\" #Tramo gen\u00e9rico de URL que contiene los datos\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0\",\n    \"Accept\": \"application/octet-stream\"\n}\n\n# Cargar checkpoint\ncheckpoint = load_checkpoint()\nREANUDACION_YEAR = checkpoint[\"year\"]\nREANUDACION_MONTH = checkpoint[\"month\"]\n\ndef get_snowflake_conn():\n    return snowflake.connector.connect(\n        user=SNOWFLAKE_USER,\n        password=SNOWFLAKE_PASSWORD,\n        account=SNOWFLAKE_ACCOUNT,\n        warehouse=SNOWFLAKE_WAREHOUSE,\n        database=SNOWFLAKE_DATABASE,\n        schema=SNOWFLAKE_SCHEMA\n    ) #retorno de la conexi\u00f3n con Snowflake\n\ndef create_table_with_constraints(conn):\n    create_table_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {SNOWFLAKE_TABLE} (\n    VENDORID NUMBER,\n    TPEP_PICKUP_DATETIME NUMBER,\n    TPEP_DROPOFF_DATETIME NUMBER,\n    PASSENGER_COUNT NUMBER,\n    TRIP_DISTANCE FLOAT,\n    RATECODEID NUMBER,\n    STORE_AND_FWD_FLAG VARCHAR(1),\n    PULOCATIONID NUMBER,\n    DOLOCATIONID NUMBER,\n    PAYMENT_TYPE NUMBER,\n    FARE_AMOUNT FLOAT,\n    EXTRA FLOAT,\n    MTA_TAX FLOAT,\n    TIP_AMOUNT FLOAT,\n    TOLLS_AMOUNT FLOAT,\n    IMPROVEMENT_SURCHARGE FLOAT,\n    TOTAL_AMOUNT FLOAT,\n    CONGESTION_SURCHARGE FLOAT,\n    AIRPORT_FEE FLOAT,\n    RUN_ID NUMBER,\n    VENTANA_TEMPORAL NUMBER,\n    LOTE_MES VARCHAR(50),\n    MONTH NUMBER,\n    YEAR NUMBER,\n    PRIMARY KEY (TPEP_PICKUP_DATETIME, TPEP_DROPOFF_DATETIME, PULOCATIONID, DOLOCATIONID)\n    )\n    \"\"\"\n    cursor = conn.cursor() #Nos aseguramos idempotencia definiendo tabla con su llave primaria\n    cursor.execute(create_table_sql)\n    cursor.close()\n\n@data_loader\ndef load_and_export_data():\n    taxi_type = \"yellow\" #generamos primero la tabla de datos para taxis amarillos\n    ID = 1\n    years = range(2015, 2026) #Carga de todos los datos de 2015-2025\n    months = range(1, 13) #Carga de todos los datos de todos los meses\n     \n    conn = get_snowflake_conn() # Crear conexi\u00f3n con Snowflake\n    \n    try:\n\n        create_table_with_constraints(conn)\n\n        for year in years: #iteramos por cada a\u00f1o por cada mes\n            if REANUDACION_YEAR != 0 and year < REANUDACION_YEAR:\n                print(f\"Saltando a\u00f1o {year} (ya procesado)\")\n                continue\n            for month in months:\n                if REANUDACION_YEAR == year and month <= REANUDACION_MONTH:\n                    print(f\"Saltando {year}-{month:02d} (ya procesado)\")\n                    continue\n                if year==2025 and month>7:\n                    break #Hasta aqui hay datos en la pagina de NY\n                file_name = f\"{taxi_type}_tripdata_{year}-{month:02d}.parquet\"\n                url = BASE_URL + file_name #URL para cada mes para cada a\u00f1o\n\n                print(f\"Descargando archivo parquet de: {url}\")\n                resp = requests.get(url, headers=HEADERS, timeout=60)\n                resp.raise_for_status()\n\n                parquet_file = pq.ParquetFile(BytesIO(resp.content)) #Lectura de datos de parquet\n                lote = 1\n\n                for batch in parquet_file.iter_batches(batch_size=1000000): #Iteraci\u00f3n por batches de datos para procesamiento sin hacer kill a la RAM\n                    df = batch.to_pandas()\n\n                    # Agregar metadatos solicitados en Deber para esquema Bronce\n                    df['run_id'] = ID\n                    df['ventana_temporal'] = datetime.now()\n                    df['lote_mes'] = f\"{lote}/{month}\"\n                    df['month'] = month\n                    df['year'] = year\n\n                    # Eliminar columnas problem\u00e1ticas directamente\n\n                    if 'cbd_congestion_fee' in df.columns:\n                        df = df.drop(columns=['cbd_congestion_fee']) #Eliminar columna que solo existe en pocos archivos\n\n                    df.columns = [c.upper() for c in df.columns] #Homogeneizar nombres de columnas\n\n                    # Exportar directamente datos a tabla de Snowflake\n                    from snowflake.connector.pandas_tools import write_pandas\n                    write_pandas(\n                        conn=conn,\n                        df=df,\n                        table_name=SNOWFLAKE_TABLE,\n                        auto_create_table=False,\n                        overwrite=False,\n                        quote_identifiers=True\n                    )\n\n                    \n                    print(f\"{file_name}: lote {lote}, {len(df)} filas exportadas correctamente\")\n                    lote += 1\n\n                # Guardar checkpoint despu\u00e9s de cada mes\n                save_checkpoint(year, month)\n                print(f\"Checkpoint guardado: {year}-{month:02d}\")\n                ID += 1\n\n    except Exception as e:\n        print(f\"Error en carga de datos: {e}\")\n    finally:\n        conn.close()\n\n    return \"Procesamiento completado. Todo lo de taxis se encuentra en Snowflake\"", "file_path": "data_loaders/loader_and_exporter_taxis_yellow.py", "language": "python", "type": "data_loader", "uuid": "loader_and_exporter_taxis_yellow"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "dbts/transformer_dbt_taxis.yaml:dbt:yaml:transformer dbt taxis": {"content": "", "file_path": "dbts/transformer_dbt_taxis.yaml", "language": "yaml", "type": "dbt", "uuid": "transformer_dbt_taxis"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/transformer_taxis_gold.py:transformer:python:transformer taxis gold": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport subprocess\nimport os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@transformer\ndef execute_dbt(*args, **kwargs):\n    \n    env_vars = {\n        'SNOWFLAKE_ACCOUNT': get_secret_value('ACCOUNT_ID'),\n        'SNOWFLAKE_USER': get_secret_value('SNOWFLAKE_USER_TRANSFORMER'),\n        'SNOWFLAKE_PASSWORD': get_secret_value('SNOWFLAKE_PASS'),\n        'SNOWFLAKE_WAREHOUSE': get_secret_value('WAREHOUSE'),\n        'SNOWFLAKE_DATABASE': get_secret_value('DATABASE'),\n        'SNOWFLAKE_SCHEMA': get_secret_value('SCHEMA'),\n        'SNOWFLAKE_ROLE_LOW_PRIVILEDGES': get_secret_value('ROLE_LOW_PRIVILEDGES'),\n        'DBT_PROFILES_DIR': '/home/src/scheduler/dbt'\n    }\n    \n    # Comandos dbt\n    commands = [\n        'dbt deps --profiles-dir /home/src/scheduler/dbt',\n        'dbt run --select tag:gold --profiles-dir /home/src/scheduler/dbt'\n    ]\n    \n    results = []\n    for cmd in commands:\n        try:\n            result = subprocess.run(\n                cmd.split(),\n                cwd='/home/src/scheduler/dbt/NY_TAXI_DBT',\n                env={**os.environ.copy(), **env_vars},\n                capture_output=True,\n                text=True\n            )\n            results.append({\n                'command': cmd,\n                'success': result.returncode == 0,\n                'output': result.stdout\n            })\n        except Exception as e:\n            results.append({\n                'command': cmd,\n                'success': False,\n                'error': str(e)\n            })\n    \n    return results", "file_path": "transformers/transformer_taxis_gold.py", "language": "python", "type": "transformer", "uuid": "transformer_taxis_gold"}, "transformers/transformer_taxis_silver.py:transformer:python:transformer taxis silver": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport subprocess\nimport os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@transformer\ndef execute_dbt(*args, **kwargs):\n    \n    env_vars = {\n        'SNOWFLAKE_ACCOUNT': get_secret_value('ACCOUNT_ID'),\n        'SNOWFLAKE_USER': get_secret_value('SNOWFLAKE_USER_TRANSFORMER'),\n        'SNOWFLAKE_PASSWORD': get_secret_value('SNOWFLAKE_PASS'),\n        'SNOWFLAKE_WAREHOUSE': get_secret_value('WAREHOUSE'),\n        'SNOWFLAKE_DATABASE': get_secret_value('DATABASE'),\n        'SNOWFLAKE_SCHEMA': get_secret_value('SCHEMA'),\n        'SNOWFLAKE_ROLE_LOW_PRIVILEDGES': get_secret_value('ROLE_LOW_PRIVILEDGES'),\n        'DBT_PROFILES_DIR': '/home/src/scheduler/dbt'\n    }\n    \n    # Comandos dbt\n    commands = [\n        'dbt deps --profiles-dir /home/src/scheduler/dbt',\n        'dbt run --select tag:silver --profiles-dir /home/src/scheduler/dbt',\n        'dbt test --select tag:silver --profiles-dir /home/src/scheduler/dbt'\n    ]\n    \n    results = []\n    for cmd in commands:\n        try:\n            result = subprocess.run(\n                cmd.split(),\n                cwd='/home/src/scheduler/dbt/NY_TAXI_DBT',\n                env={**os.environ.copy(), **env_vars},\n                capture_output=True,\n                text=True\n            )\n            results.append({\n                'command': cmd,\n                'success': result.returncode == 0,\n                'output': result.stdout\n            })\n        except Exception as e:\n            results.append({\n                'command': cmd,\n                'success': False,\n                'error': str(e)\n            })\n    \n    return results", "file_path": "transformers/transformer_taxis_silver.py", "language": "python", "type": "transformer", "uuid": "transformer_taxis_silver"}, "transformers/transformer_taxis_yellow_bronze.py:transformer:python:transformer taxis yellow bronze": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport subprocess\nimport os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@transformer\ndef execute_dbt(*args, **kwargs):\n    \n    env_vars = {\n        'SNOWFLAKE_ACCOUNT': get_secret_value('ACCOUNT_ID'),\n        'SNOWFLAKE_USER': get_secret_value('SNOWFLAKE_USER_TRANSFORMER'),\n        'SNOWFLAKE_PASSWORD': get_secret_value('SNOWFLAKE_PASS'),\n        'SNOWFLAKE_WAREHOUSE': get_secret_value('WAREHOUSE'),\n        'SNOWFLAKE_DATABASE': get_secret_value('DATABASE'),\n        'SNOWFLAKE_SCHEMA': get_secret_value('SCHEMA'),\n        'SNOWFLAKE_ROLE_LOW_PRIVILEDGES': get_secret_value('ROLE_LOW_PRIVILEDGES'),\n        'DBT_PROFILES_DIR': '/home/src/scheduler/dbt'\n    }\n    \n    # Comandos dbt\n    commands = [\n        'dbt deps --profiles-dir /home/src/scheduler/dbt',\n        'dbt run --select tag:bronze --profiles-dir /home/src/scheduler/dbt'\n    ]\n    \n    results = []\n    for cmd in commands:\n        try:\n            result = subprocess.run(\n                cmd.split(),\n                cwd='/home/src/scheduler/dbt/NY_TAXI_DBT',\n                env={**os.environ.copy(), **env_vars},\n                capture_output=True,\n                text=True\n            )\n            results.append({\n                'command': cmd,\n                'success': result.returncode == 0,\n                'output': result.stdout\n            })\n        except Exception as e:\n            results.append({\n                'command': cmd,\n                'success': False,\n                'error': str(e)\n            })\n    \n    return results", "file_path": "transformers/transformer_taxis_yellow_bronze.py", "language": "python", "type": "transformer", "uuid": "transformer_taxis_yellow_bronze"}, "transformers/trnsoford.py:transformer:python:trnsoford": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n# In your transformation block\n@transformer\ndef transform_data(df, *args, **kwargs):\n    # df will be a single DataFrame from one of the yielded chunks\n    # Process your data here\n    return df\n", "file_path": "transformers/trnsoford.py", "language": "python", "type": "transformer", "uuid": "trnsoford"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/pipeline_taxis_nyc/metadata.yaml:pipeline:yaml:pipeline taxis nyc/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/loader_and_exporter_taxis_yellow.py\n    file_source:\n      path: data_loaders/loader_and_exporter_taxis_yellow.py\n  downstream_blocks:\n  - loader_and_exporter_taxis_green\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: loader_and_exporter_taxis_yellow\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: loader_and_exporter_taxis_yellow\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_loaders/loader_and_exporter_taxis_green.py\n    file_source:\n      path: data_loaders/loader_and_exporter_taxis_green.py\n  downstream_blocks:\n  - dataloader_taxizones\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: loader_and_exporter_taxis_green\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks:\n  - loader_and_exporter_taxis_yellow\n  uuid: loader_and_exporter_taxis_green\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - exporter_taxizones\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dataloader_taxizones\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks:\n  - loader_and_exporter_taxis_green\n  uuid: dataloader_taxizones\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - transformer_taxis_yellow_bronze\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: exporter_taxizones\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - dataloader_taxizones\n  uuid: exporter_taxizones\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - transformer_taxis_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transformer_taxis_yellow_bronze\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - exporter_taxizones\n  uuid: transformer_taxis_yellow_bronze\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - transformer_taxis_gold\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transformer_taxis_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - transformer_taxis_yellow_bronze\n  uuid: transformer_taxis_silver\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transformer_taxis_gold\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - transformer_taxis_silver\n  uuid: transformer_taxis_gold\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-19 05:20:00.999661+00:00'\ndata_integration: null\ndescription: pipeline_taxis_nyc\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: pipeline_taxis_nyc\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: pipeline_taxis_nyc\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/pipeline_taxis_nyc/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "pipeline_taxis_nyc/metadata"}, "pipelines/pipeline_taxis_nyc/__init__.py:pipeline:python:pipeline taxis nyc/  init  ": {"content": "", "file_path": "pipelines/pipeline_taxis_nyc/__init__.py", "language": "python", "type": "pipeline", "uuid": "pipeline_taxis_nyc/__init__"}, "/home/src/scheduler/data_exporters/exporter_taxizones.py:data_exporter:python:home/src/scheduler/data exporters/exporter taxizones": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom pandas import DataFrame\nfrom os import path\nfrom snowflake.connector.pandas_tools import write_pandas\nimport snowflake.connector\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nSNOWFLAKE_ACCOUNT = get_secret_value('ACCOUNT_ID')\nSNOWFLAKE_USER = get_secret_value('SNOWFLAKE_USER_TRANSFORMER')\nSNOWFLAKE_PRIVATE_KEY = get_secret_value('RSA_PRIVATE')\nSNOWFLAKE_DATABASE = get_secret_value('DATABASE')\nSNOWFLAKE_SCHEMA = get_secret_value('SCHEMA')\nSNOWFLAKE_WAREHOUSE = get_secret_value('WAREHOUSE')\nSNOWFLAKE_TABLE = 'NEWYORK_TAXIS_ZONES3'\n\ndef get_snowflake_conn():\n    return snowflake.connector.connect(\n        user=SNOWFLAKE_USER,\n        private_key=SNOWFLAKE_PRIVATE_KEY,\n        account=SNOWFLAKE_ACCOUNT,\n        warehouse=SNOWFLAKE_WAREHOUSE,\n        database=SNOWFLAKE_DATABASE,\n        schema=SNOWFLAKE_SCHEMA\n    ) #retorno de la conexi\u00f3n con Snowflake\n\n@data_exporter\ndef export_data_to_snowflake(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Snowflake warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#snowflake\n    \"\"\"\n\n    try:\n        conn = get_snowflake_conn() \n        write_pandas(conn=conn,df=df,table_name=SNOWFLAKE_TABLE,auto_create_table=True)\n        print(\"Data enviada correctamente a Snowflake para Zonas de Taxis de NY\")\n\n    except Exception as e:\n        print(f\"Error al cargar el archivo a Snowflake: {e}\")\n        raise\n", "file_path": "/home/src/scheduler/data_exporters/exporter_taxizones.py", "language": "python", "type": "data_exporter", "uuid": "exporter_taxizones"}, "/home/src/scheduler/transformers/transformer_taxis_gold.py:transformer:python:home/src/scheduler/transformers/transformer taxis gold": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport subprocess\nimport os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@transformer\ndef execute_dbt(*args, **kwargs):\n    \n    env_vars = {\n        'SNOWFLAKE_ACCOUNT': get_secret_value('ACCOUNT_ID'),\n        'SNOWFLAKE_USER': get_secret_value('SNOWFLAKE_USER_TRANSFORMER'),\n        'SNOWFLAKE_PRIVATE_KEY': get_secret_value('RSA_PRIVATE'),\n        'SNOWFLAKE_WAREHOUSE': get_secret_value('WAREHOUSE'),\n        'SNOWFLAKE_DATABASE': get_secret_value('DATABASE'),\n        'SNOWFLAKE_SCHEMA': get_secret_value('SCHEMA'),\n        'SNOWFLAKE_ROLE_LOW_PRIVILEDGES': get_secret_value('ROLE_LOW_PRIVILEDGES'),\n        'DBT_PROFILES_DIR': '/home/src/scheduler/dbt'\n    }\n    \n    # Comandos dbt\n    commands = [\n        'dbt deps --profiles-dir /home/src/scheduler/dbt',\n        'dbt run --select tag:gold --profiles-dir /home/src/scheduler/dbt',\n        'dbt test --select tag:gold --profiles-dir /home/src/scheduler/dbt'\n    ]\n    \n    results = []\n    for cmd in commands:\n        try:\n            result = subprocess.run(\n                cmd.split(),\n                cwd='/home/src/scheduler/dbt/NY_TAXI_DBT',\n                env={**os.environ.copy(), **env_vars},\n                capture_output=True,\n                text=True\n            )\n            results.append({\n                'command': cmd,\n                'success': result.returncode == 0,\n                'output': result.stdout\n            })\n        except Exception as e:\n            results.append({\n                'command': cmd,\n                'success': False,\n                'error': str(e)\n            })\n    \n    return results", "file_path": "/home/src/scheduler/transformers/transformer_taxis_gold.py", "language": "python", "type": "transformer", "uuid": "transformer_taxis_gold"}, "/home/src/scheduler/transformers/transformer_taxis_yellow_bronze.py:transformer:python:home/src/scheduler/transformers/transformer taxis yellow bronze": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport subprocess\nimport os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@transformer\ndef execute_dbt(*args, **kwargs):\n    \n    env_vars = {\n        'SNOWFLAKE_ACCOUNT': get_secret_value('ACCOUNT_ID'),\n        'SNOWFLAKE_USER': get_secret_value('SNOWFLAKE_USER_TRANSFORMER'),\n        'SNOWFLAKE_PRIVATE_KEY': get_secret_value('RSA_PRIVATE'),\n        'SNOWFLAKE_WAREHOUSE': get_secret_value('WAREHOUSE'),\n        'SNOWFLAKE_DATABASE': get_secret_value('DATABASE'),\n        'SNOWFLAKE_SCHEMA': get_secret_value('SCHEMA'),\n        'SNOWFLAKE_ROLE_LOW_PRIVILEDGES': get_secret_value('ROLE_LOW_PRIVILEDGES'),\n        'DBT_PROFILES_DIR': '/home/src/scheduler/dbt'\n    }\n    \n    # Comandos dbt\n    commands = [\n        'dbt deps --profiles-dir /home/src/scheduler/dbt',\n        'dbt run --select tag:bronze --profiles-dir /home/src/scheduler/dbt'\n    ]\n    \n    results = []\n    for cmd in commands:\n        try:\n            result = subprocess.run(\n                cmd.split(),\n                cwd='/home/src/scheduler/dbt/NY_TAXI_DBT',\n                env={**os.environ.copy(), **env_vars},\n                capture_output=True,\n                text=True\n            )\n            results.append({\n                'command': cmd,\n                'success': result.returncode == 0,\n                'output': result.stdout\n            })\n        except Exception as e:\n            results.append({\n                'command': cmd,\n                'success': False,\n                'error': str(e)\n            })\n    \n    return results", "file_path": "/home/src/scheduler/transformers/transformer_taxis_yellow_bronze.py", "language": "python", "type": "transformer", "uuid": "transformer_taxis_yellow_bronze"}, "/home/src/scheduler/transformers/transformer_taxis_silver.py:transformer:python:home/src/scheduler/transformers/transformer taxis silver": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport subprocess\nimport os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n@transformer\ndef execute_dbt(*args, **kwargs):\n    \n    env_vars = {\n        'SNOWFLAKE_ACCOUNT': get_secret_value('ACCOUNT_ID'),\n        'SNOWFLAKE_USER': get_secret_value('SNOWFLAKE_USER_TRANSFORMER'),\n        'SNOWFLAKE_PRIVATE_KEY': get_secret_value('RSA_PRIVATE'),\n        'SNOWFLAKE_WAREHOUSE': get_secret_value('WAREHOUSE'),\n        'SNOWFLAKE_DATABASE': get_secret_value('DATABASE'),\n        'SNOWFLAKE_SCHEMA': get_secret_value('SCHEMA'),\n        'SNOWFLAKE_ROLE_LOW_PRIVILEDGES': get_secret_value('ROLE_LOW_PRIVILEDGES'),\n        'DBT_PROFILES_DIR': '/home/src/scheduler/dbt'\n    }\n    \n    # Comandos dbt\n    commands = [\n        'dbt deps --profiles-dir /home/src/scheduler/dbt',\n        'dbt run --select tag:silver --profiles-dir /home/src/scheduler/dbt',\n        'dbt test --select tag:silver --profiles-dir /home/src/scheduler/dbt'\n    ]\n    \n    results = []\n    for cmd in commands:\n        try:\n            result = subprocess.run(\n                cmd.split(),\n                cwd='/home/src/scheduler/dbt/NY_TAXI_DBT',\n                env={**os.environ.copy(), **env_vars},\n                capture_output=True,\n                text=True\n            )\n            results.append({\n                'command': cmd,\n                'success': result.returncode == 0,\n                'output': result.stdout\n            })\n        except Exception as e:\n            results.append({\n                'command': cmd,\n                'success': False,\n                'error': str(e)\n            })\n    \n    return results", "file_path": "/home/src/scheduler/transformers/transformer_taxis_silver.py", "language": "python", "type": "transformer", "uuid": "transformer_taxis_silver"}, "/home/src/scheduler/data_loaders/loader_and_exporter_taxis_green.py:data_loader:python:home/src/scheduler/data loaders/loader and exporter taxis green": {"content": "import pandas as pd\nimport requests\nfrom io import BytesIO\nimport pyarrow.parquet as pq\nfrom datetime import datetime\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nimport json\nimport os\n\nCHECKPOINT_FILE = \"checkpointTaxisGreen.json\"\n\ndef save_checkpoint(year, month):\n    with open(CHECKPOINT_FILE, \"w\") as f:\n        json.dump({\"year\": year, \"month\": month}, f)\n\ndef load_checkpoint():\n    if os.path.exists(CHECKPOINT_FILE):\n        with open(CHECKPOINT_FILE, \"r\") as f:\n            return json.load(f)\n    return {\"year\": 0, \"month\": 0}\n\n# Datos para la conexi\u00f3n con Snowflake. Ver si se pueden poner en el io_config.yaml\nSNOWFLAKE_ACCOUNT = get_secret_value('ACCOUNT_ID')\nSNOWFLAKE_USER = get_secret_value('SNOWFLAKE_USER_TRANSFORMER')\nSNOWFLAKE_PRIVATE_KEY = get_secret_value('RSA_PRIVATE')\nSNOWFLAKE_DATABASE = get_secret_value('DATABASE')\nSNOWFLAKE_SCHEMA = get_secret_value('SCHEMA')\nSNOWFLAKE_WAREHOUSE = get_secret_value('WAREHOUSE')\nSNOWFLAKE_TABLE = 'NY_TAXIS_GREEN_BRONZE' \n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\" #Tramo gen\u00e9rico de URL que contiene los datos\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0\",\n    \"Accept\": \"application/octet-stream\"\n}\n\n# Cargar checkpoint\ncheckpoint = load_checkpoint()\nREANUDACION_YEAR = checkpoint[\"year\"]\nREANUDACION_MONTH = checkpoint[\"month\"]\n\ndef get_snowflake_conn():\n    return snowflake.connector.connect(\n        user=SNOWFLAKE_USER,\n        private_key=SNOWFLAKE_PRIVATE_KEY,\n        account=SNOWFLAKE_ACCOUNT,\n        warehouse=SNOWFLAKE_WAREHOUSE,\n        database=SNOWFLAKE_DATABASE,\n        schema=SNOWFLAKE_SCHEMA\n    ) #retorno de la conexi\u00f3n con Snowflake\n\ndef create_table_with_constraints(conn):\n    create_table_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {SNOWFLAKE_TABLE} (\n    VENDORID NUMBER,\n    LPEP_PICKUP_DATETIME NUMBER,\n    LPEP_DROPOFF_DATETIME NUMBER,\n    PASSENGER_COUNT NUMBER,\n    TRIP_DISTANCE FLOAT,\n    RATECODEID NUMBER,\n    STORE_AND_FWD_FLAG VARCHAR(1),\n    PULOCATIONID NUMBER,\n    DOLOCATIONID NUMBER,\n    PAYMENT_TYPE NUMBER,\n    FARE_AMOUNT FLOAT,\n    EXTRA FLOAT,\n    MTA_TAX FLOAT,\n    TIP_AMOUNT FLOAT,\n    TOLLS_AMOUNT FLOAT,\n    EHAIL_FEE FLOAT,\n    IMPROVEMENT_SURCHARGE FLOAT,\n    TOTAL_AMOUNT FLOAT,\n    TRIP_TYPE NUMBER,\n    CONGESTION_SURCHARGE FLOAT,\n    AIRPORT_FEE FLOAT,\n    RUN_ID NUMBER,\n    VENTANA_TEMPORAL NUMBER,\n    LOTE_MES VARCHAR(50),\n    MONTH NUMBER,\n    YEAR NUMBER,\n    PRIMARY KEY (LPEP_PICKUP_DATETIME, LPEP_DROPOFF_DATETIME, PULOCATIONID, DOLOCATIONID)\n    )\n    \"\"\"\n    cursor = conn.cursor() #Nos aseguramos idempotencia definiendo tabla con su llave primaria\n    cursor.execute(create_table_sql)\n    cursor.close()\n\n@data_loader\ndef load_and_export_data(output_yellow):\n    taxi_type = \"green\" #generamos segundo la tabla para taxis verdes\n    ID = 1\n    years = range(2015, 2026) #Carga de todos los datos de 2015-2025\n    months = range(1, 13) #Carga de todos los datos de todos los meses\n     \n    conn = get_snowflake_conn() # Crear conexi\u00f3n con Snowflake\n    \n    try:\n\n        create_table_with_constraints(conn)\n\n        for year in years: #iteramos por cada a\u00f1o por cada mes\n            if REANUDACION_YEAR != 0 and year < REANUDACION_YEAR:\n                print(f\"Saltando a\u00f1o {year} (ya procesado)\")\n                continue\n            for month in months:\n                if REANUDACION_YEAR == year and month <= REANUDACION_MONTH:\n                    print(f\"Saltando {year}-{month:02d} (ya procesado)\")\n                    continue\n                if year==2025 and month>7:\n                    break #Hasta aqui hay datos en la pagina de NEW YORK\n                file_name = f\"{taxi_type}_tripdata_{year}-{month:02d}.parquet\"\n                url = BASE_URL + file_name #URL para cada mes para cada a\u00f1o\n\n                print(f\"Descargando archivo parquet de: {url}\")\n                resp = requests.get(url, headers=HEADERS, timeout=60)\n                resp.raise_for_status()\n\n                parquet_file = pq.ParquetFile(BytesIO(resp.content)) #Lectura de datos de parquet\n                lote = 1\n\n                for batch in parquet_file.iter_batches(batch_size=1000000): #Iteraci\u00f3n por batches de datos para procesamiento sin hacer kill a la RAM\n                    df = batch.to_pandas()\n\n                    # Agregar metadatos solicitados en Deber para esquema Bronce\n                    df['run_id'] = ID\n                    df['ventana_temporal'] = datetime.now()\n                    df['lote_mes'] = f\"{lote}/{month}\"\n                    df['month'] = month\n                    df['year'] = year\n\n                    # Eliminar columnas problem\u00e1ticas directamente\n\n                    if 'cbd_congestion_fee' in df.columns:\n                        df = df.drop(columns=['cbd_congestion_fee']) #Eliminar columna que solo existe en pocos archivos\n\n                    df.columns = [c.upper() for c in df.columns] #Homogeneizar nombres de columnas\n                    \n                    # Exportar directamente datos a tabla de Snowflake\n                    from snowflake.connector.pandas_tools import write_pandas\n                    write_pandas(\n                        conn=conn,\n                        df=df,\n                        table_name=SNOWFLAKE_TABLE,\n                        auto_create_table=True\n                    )\n\n                    print(f\"{file_name}: lote {lote}, {len(df)} filas exportadas correctamente\")\n                    lote += 1\n\n                # Guardar checkpoint despu\u00e9s de cada mes\n                save_checkpoint(year, month)\n                print(f\"Checkpoint guardado: {year}-{month:02d}\")\n                ID += 1\n\n    except Exception as e:\n        print(f\"Error en carga de datos: {e}\")\n    finally:\n        conn.close()\n\n    return \"Procesamiento completado. Todo lo de taxis se encuentra en Snowflake\"", "file_path": "/home/src/scheduler/data_loaders/loader_and_exporter_taxis_green.py", "language": "python", "type": "data_loader", "uuid": "loader_and_exporter_taxis_green"}, "/home/src/scheduler/data_loaders/loader_and_exporter_taxis_yellow.py:data_loader:python:home/src/scheduler/data loaders/loader and exporter taxis yellow": {"content": "import pandas as pd\nimport requests\nfrom io import BytesIO\nimport pyarrow.parquet as pq\nfrom datetime import datetime\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\nimport json\nimport os\n\nCHECKPOINT_FILE = \"checkpointTaxisYellow.json\"\n\ndef save_checkpoint(year, month):\n    with open(CHECKPOINT_FILE, \"w\") as f:\n        json.dump({\"year\": year, \"month\": month}, f)\n\ndef load_checkpoint():\n    if os.path.exists(CHECKPOINT_FILE):\n        with open(CHECKPOINT_FILE, \"r\") as f:\n            return json.load(f)\n    return {\"year\": 0, \"month\": 0}\n\n# Datos para la conexi\u00f3n con Snowflake. Ver si se pueden poner en el io_config.yaml\nSNOWFLAKE_ACCOUNT = get_secret_value('ACCOUNT_ID')\nSNOWFLAKE_USER = get_secret_value('SNOWFLAKE_USER_TRANSFORMER')\nSNOWFLAKE_PRIVATE_KEY = get_secret_value('RSA_PRIVATE')\nSNOWFLAKE_DATABASE = get_secret_value('DATABASE')\nSNOWFLAKE_SCHEMA = get_secret_value('SCHEMA')\nSNOWFLAKE_WAREHOUSE = get_secret_value('WAREHOUSE')\nSNOWFLAKE_TABLE = 'NY_TAXIS_YELLOW_BRONZE' \n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\" #Tramo gen\u00e9rico de URL que contiene los datos\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0\",\n    \"Accept\": \"application/octet-stream\"\n}\n\n# Cargar checkpoint\ncheckpoint = load_checkpoint()\nREANUDACION_YEAR = checkpoint[\"year\"]\nREANUDACION_MONTH = checkpoint[\"month\"]\n\ndef get_snowflake_conn():\n    return snowflake.connector.connect(\n        user=SNOWFLAKE_USER,\n        private_key=SNOWFLAKE_PRIVATE_KEY,\n        account=SNOWFLAKE_ACCOUNT,\n        warehouse=SNOWFLAKE_WAREHOUSE,\n        database=SNOWFLAKE_DATABASE,\n        schema=SNOWFLAKE_SCHEMA\n    ) #retorno de la conexi\u00f3n con Snowflake\n\ndef create_table_with_constraints(conn):\n    create_table_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {SNOWFLAKE_TABLE} (\n    VENDORID NUMBER,\n    TPEP_PICKUP_DATETIME NUMBER,\n    TPEP_DROPOFF_DATETIME NUMBER,\n    PASSENGER_COUNT NUMBER,\n    TRIP_DISTANCE FLOAT,\n    RATECODEID NUMBER,\n    STORE_AND_FWD_FLAG VARCHAR(1),\n    PULOCATIONID NUMBER,\n    DOLOCATIONID NUMBER,\n    PAYMENT_TYPE NUMBER,\n    FARE_AMOUNT FLOAT,\n    EXTRA FLOAT,\n    MTA_TAX FLOAT,\n    TIP_AMOUNT FLOAT,\n    TOLLS_AMOUNT FLOAT,\n    IMPROVEMENT_SURCHARGE FLOAT,\n    TOTAL_AMOUNT FLOAT,\n    CONGESTION_SURCHARGE FLOAT,\n    AIRPORT_FEE FLOAT,\n    RUN_ID NUMBER,\n    VENTANA_TEMPORAL NUMBER,\n    LOTE_MES VARCHAR(50),\n    MONTH NUMBER,\n    YEAR NUMBER,\n    PRIMARY KEY (TPEP_PICKUP_DATETIME, TPEP_DROPOFF_DATETIME, PULOCATIONID, DOLOCATIONID)\n    )\n    \"\"\"\n    cursor = conn.cursor() #Nos aseguramos idempotencia definiendo tabla con su llave primaria\n    cursor.execute(create_table_sql)\n    cursor.close()\n\n@data_loader\ndef load_and_export_data():\n    taxi_type = \"yellow\" #generamos primero la tabla de datos para taxis amarillos\n    ID = 1\n    years = range(2015, 2026) #Carga de todos los datos de 2015-2025\n    months = range(1, 13) #Carga de todos los datos de todos los meses\n     \n    conn = get_snowflake_conn() # Crear conexi\u00f3n con Snowflake\n    \n    try:\n\n        create_table_with_constraints(conn)\n\n        for year in years: #iteramos por cada a\u00f1o por cada mes\n            if REANUDACION_YEAR != 0 and year < REANUDACION_YEAR:\n                print(f\"Saltando a\u00f1o {year} (ya procesado)\")\n                continue\n            for month in months:\n                if REANUDACION_YEAR == year and month <= REANUDACION_MONTH:\n                    print(f\"Saltando {year}-{month:02d} (ya procesado)\")\n                    continue\n                if year==2025 and month>7:\n                    break #Hasta aqui hay datos en la pagina de NY\n                file_name = f\"{taxi_type}_tripdata_{year}-{month:02d}.parquet\"\n                url = BASE_URL + file_name #URL para cada mes para cada a\u00f1o\n\n                print(f\"Descargando archivo parquet de: {url}\")\n                resp = requests.get(url, headers=HEADERS, timeout=60)\n                resp.raise_for_status()\n\n                parquet_file = pq.ParquetFile(BytesIO(resp.content)) #Lectura de datos de parquet\n                lote = 1\n\n                for batch in parquet_file.iter_batches(batch_size=1000000): #Iteraci\u00f3n por batches de datos para procesamiento sin hacer kill a la RAM\n                    df = batch.to_pandas()\n\n                    # Agregar metadatos solicitados en Deber para esquema Bronce\n                    df['run_id'] = ID\n                    df['ventana_temporal'] = datetime.now()\n                    df['lote_mes'] = f\"{lote}/{month}\"\n                    df['month'] = month\n                    df['year'] = year\n\n                    # Eliminar columnas problem\u00e1ticas directamente\n\n                    if 'cbd_congestion_fee' in df.columns:\n                        df = df.drop(columns=['cbd_congestion_fee']) #Eliminar columna que solo existe en pocos archivos\n\n                    df.columns = [c.upper() for c in df.columns] #Homogeneizar nombres de columnas\n\n                    # Exportar directamente datos a tabla de Snowflake\n                    from snowflake.connector.pandas_tools import write_pandas\n                    write_pandas(\n                        conn=conn,\n                        df=df,\n                        table_name=SNOWFLAKE_TABLE,\n                        auto_create_table=False,\n                        overwrite=False,\n                        quote_identifiers=True\n                    )\n\n                    \n                    print(f\"{file_name}: lote {lote}, {len(df)} filas exportadas correctamente\")\n                    lote += 1\n\n                # Guardar checkpoint despu\u00e9s de cada mes\n                save_checkpoint(year, month)\n                print(f\"Checkpoint guardado: {year}-{month:02d}\")\n                ID += 1\n\n    except Exception as e:\n        print(f\"Error en carga de datos: {e}\")\n    finally:\n        conn.close()\n\n    return \"Procesamiento completado. Todo lo de taxis se encuentra en Snowflake\"", "file_path": "/home/src/scheduler/data_loaders/loader_and_exporter_taxis_yellow.py", "language": "python", "type": "data_loader", "uuid": "loader_and_exporter_taxis_yellow"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}